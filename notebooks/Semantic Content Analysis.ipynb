{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a976ca6",
   "metadata": {},
   "source": [
    "# Twitter Automation Scanner - Semantic Content analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59e6e96",
   "metadata": {},
   "source": [
    "## Install required dependencies\n",
    "\n",
    "- [langdetect](https://github.com/Mimino666/langdetect) - langugage detection library\n",
    "- [matplotlib](https://matplotlib.org) - Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python. Matplotlib makes easy things easy and hard things possible.\n",
    "- [wordcloud](https://github.com/amueller/word_cloud) - A little word cloud generator in Python\n",
    "- [spaCy](https://spacy.io) - spaCy is a library for advanced Natural Language Processing. spaCy comes with pretrained pipelines and currently supports tokenization and training for 70+ languages. It features state-of-the-art speed and neural network models for tagging, parsing, named entity recognition, text classification and more, multi-task learning with pretrained transformers.\n",
    "- [NLTK](https://www.nltk.org) - NLTK stands for Natural Language Toolkit. NLTK is a platform for building Python programs to work with human language data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663b19e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langdetect matplotlib wordcloud spacy nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cfc052",
   "metadata": {},
   "source": [
    "## Load source JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44abf02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "folder_walk = os.walk('sourcedata/user')\n",
    "first_file_in_folder = next(folder_walk)[2][0]\n",
    "\n",
    "report_file = open(f'{ os.getcwd() }/sourcedata/user/{first_file_in_folder}')\n",
    "user_data = json.load(report_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a04a7e9",
   "metadata": {},
   "source": [
    "## Detect the user language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47042350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect_langs\n",
    "\n",
    "separator = ' '\n",
    "tweets = list()\n",
    "\n",
    "for tweet in user_data['tweets']:\n",
    "    tweets.append(tweet['text'])\n",
    "\n",
    "merged_user_posts = separator.join(tweets)\n",
    "detected_langs_result = detect_langs(merged_user_posts)\n",
    "\n",
    "print( 'For the provided report used language probabilities are:' )\n",
    "for result in detected_langs_result:\n",
    "    probability = round(result.prob * 100, 4)\n",
    "    print(f'Detected language is { result.lang } with probability { probability }%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6fc0af",
   "metadata": {},
   "source": [
    "## Define stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "771e5c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended https://github.com/bieli/stopwords/blob/master/polish.stopwords.txt\n",
    "stop_words = [\n",
    "    'a', 'aby', 'ach', 'acz', 'aczkolwiek', 'aj', 'albo', 'ale', 'alez', 'ależ', 'ani', 'az', 'aż', 'bardziej',\n",
    "    'bardzo', 'beda', 'bedzie', 'bez', 'będą', 'bede', 'będę', 'będzie', 'bo', 'bowiem', 'by', 'byc', 'być', 'byl',\n",
    "    'byla', 'byli', 'bylo', 'byly', 'był', 'była', 'było', 'były', 'bynajmniej', 'cala', 'cali', 'caly', 'cała',\n",
    "    'cały', 'ci', 'cie', 'ciebie', 'cię', 'co', 'cokolwiek', 'cos', 'coś', 'czasami', 'czasem', 'czemu', 'czy',\n",
    "    'czyli', 'czym', 'daleko', 'da', 'dla', 'dlaczego', 'dlatego', 'do', 'dobrze', 'dokad', 'dokąd', 'dosc', 'dość',\n",
    "    'duzo', 'dużo', 'dwa', 'dwaj', 'dwie', 'dwoje', 'dzis', 'dzisiaj', 'dziś', 'gdy', 'gdyby', 'gdyz', 'gdyż',\n",
    "    'gdzie', 'gdziekolwiek', 'gdzies', 'gdzieś', 'go', 'i', 'ich', 'ile', 'im', 'inna', 'inne', 'inny', 'innych',\n",
    "    'itd', 'iz', 'iż', 'ja', 'jak', 'jaka', 'jakas', 'jakaś', 'jakby', 'jaki', 'jakichs', 'jakichś', 'jakie',\n",
    "    'jakieś', 'jakis', 'jakies', 'jakiś', 'jakiz', 'jakiż', 'jakkolwiek', 'jako', 'jakos', 'jakoś', 'ją', 'je',\n",
    "    'jeden', 'jedna', 'jednak', 'jednakze', 'jednakże', 'jedno', 'jego', 'jej', 'jemu', 'jesli', 'jest', 'jestem',\n",
    "    'jeszcze', 'jeśli', 'jezeli', 'jeżeli', 'juz', 'już', 'kazdy', 'każdy', 'kiedy', 'kilka', 'kims', 'kimś', 'kto',\n",
    "    'ktokolwiek', 'ktora', 'ktore,', 'ktorego', 'ktorej', 'ktory', 'ktorych', 'ktorym', 'ktorzy', 'ktos', 'ktoś',\n",
    "    'która', 'które', 'którego', 'której', 'który', 'których', 'którym', 'którzy', 'ku', 'lat', 'lecz', 'lub',\n",
    "    'ma', 'mają', 'maja', 'mało', 'mam', 'mi', 'miedzy', 'między', 'mimo', 'mna', 'mną', 'mnie', 'moga', 'mogą',\n",
    "    'moi', 'moim', 'moj', 'moja', 'moje', 'moze', 'mozliwe', 'mozna', 'może', 'możliwe', 'mój', 'można', 'mu',\n",
    "    'musi', 'my', 'na', 'nad', 'nam', 'nami', 'nas', 'nasi', 'nasz', 'nasza', 'nasze', 'naszego', 'naszych',\n",
    "    'natomiast', 'natychmiast', 'nawet', 'nia', 'niby', 'nic', 'nią', 'nie', 'nich', 'niech', 'niego', 'niej',\n",
    "    'niemu', 'niemu', 'nigdy', 'nim', 'nimi', 'niz', 'niż', 'no', 'np', 'nr', 'o', 'obok', 'oby', 'od', 'ok',\n",
    "    'około', 'on', 'ona', 'one', 'oni', 'ono', 'oraz', 'oto', 'owszem', 'pan', 'pana', 'pani', 'po', 'pod',\n",
    "    'podczas', 'pomimo', 'ponad', 'poniewaz', 'ponieważ', 'powinien', 'powinna', 'powinni', 'powinno', 'poza',\n",
    "    'prawie', 'przeciez', 'przecież', 'przed', 'przede', 'przedtem', 'przez', 'przy', 'roku', 'rowniez',\n",
    "    'również', 'rt', 'sam', 'sama', 'są', 'sie', 'się', 'skad', 'skąd', 'soba', 'sobą', 'sobie', 'sposob',\n",
    "    'sposób', 'swoje', 't', 'tt', 'ta', 'tak', 'taka', 'taki', 'takie', 'takze', 'także', 'tam', 'te', 'tę', \n",
    "    'tego', 'tej', 'ten', 'teraz', 'też', 'to', 'toba', 'tobą', 'tobie', 'totez', 'toteż', 'totobą', 'trzeba',\n",
    "    'tu', 'tutaj', 'twoi', 'twoim', 'twoj', 'twoja', 'twoje', 'twój', 'twym', 'ty', 'tych', 'tylko', 'tym',\n",
    "    'u', 'w', 'wam', 'wami', 'was', 'wasz', 'wasza', 'wasze', 'we', 'według', 'wiele', 'wielu', 'więc', 'więcej',\n",
    "    'wlasnie', 'właśnie', 'ws', 'wszyscy', 'wszystkich', 'wszystkie', 'wszystkim', 'wszystko', 'wtedy', 'wy', 'z', 'za',\n",
    "    'zaden', 'zadna', 'zadne', 'zadnych', 'zapewne', 'zawsze', 'ze', 'zeby', 'zeznowu', 'zł', 'znow', 'znowu',\n",
    "    'znów', 'zostal', 'został', 'żaden', 'żadna', 'żadne', 'żadnych', 'że', 'żeby', ' ', 'itp', 'cc', 'ów', 'mieć'\n",
    "    'miał', 'miała', 'mieli', 'tj', 'xd', 'owe', 'temu', 'tych', 'zaś', 'zatem'\n",
    "] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dbc2be",
   "metadata": {},
   "source": [
    "## Prepare a word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c96871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "def black_color_func(word, font_size, position,orientation,random_state=None, **kwargs):\n",
    "    return(\"hsl(0,100%, 1%)\")\n",
    "\n",
    "merged_tweets = separator.join(tweets).lower()\n",
    "\n",
    "merged_tweets = re.sub(r'http\\S+', '', merged_tweets)\n",
    "\n",
    "words = merged_tweets.split()\n",
    "\n",
    "trimmed_words = list()\n",
    "\n",
    "for word in words:\n",
    "    word = re.sub('\\W+','', word)\n",
    "    if word not in stop_words and not word.startswith('@'):\n",
    "        trimmed_words.append( word )\n",
    "\n",
    "text = ' '.join(trimmed_words)\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\", width=3000, height=2000).generate(text)\n",
    "\n",
    "wordcloud.recolor(color_func = black_color_func)\n",
    "\n",
    "plt.figure(figsize=[10, 15])\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd880cf",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4c5703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load(\"pl_core_news_lg\")\n",
    "\n",
    "content = nlp(merged_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746403ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# render text with marked named entities\n",
    "displacy.render(content, jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6220b7",
   "metadata": {},
   "source": [
    "## Count lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258e1542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "\n",
    "lemmas = list()\n",
    "\n",
    "content = nlp(text)\n",
    "\n",
    "for token in content:\n",
    "  lemmas.append(token.lemma_)\n",
    "\n",
    "print('Lemmas count for user is:', len(Counter(lemmas).items()))\n",
    "print('Analyzed tweets count: ', len(tweets))\n",
    "print('Total count of words (excl. stop words):', len(trimmed_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e4b9de",
   "metadata": {},
   "source": [
    "## Discover collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334676ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "finder = BigramCollocationFinder.from_words(lemmas)\n",
    "bgm = BigramAssocMeasures()\n",
    "collocations = {bigram: pmi for bigram, pmi in finder.score_ngrams(bgm.mi_like)}\n",
    "\n",
    "collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134164d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475726ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
